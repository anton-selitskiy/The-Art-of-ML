{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "39c2fe83",
      "metadata": {
        "id": "39c2fe83"
      },
      "outputs": [],
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Is this notebook running on Colab or Kaggle?\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "if not tf.config.list_physical_devices('GPU'):\n",
        "    print(\"No GPU was detected. CNNs can be very slow without a GPU.\")\n",
        "    if IS_COLAB:\n",
        "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
        "    if IS_KAGGLE:\n",
        "        print(\"Go to Settings > Accelerator and select GPU.\")\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Download and Prepare"
      ],
      "metadata": {
        "id": "D4DxdnYKQc92"
      },
      "id": "D4DxdnYKQc92"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "8fd517a9",
      "metadata": {
        "id": "8fd517a9"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "dataset, info = tfds.load(\"tf_flowers\", as_supervised=True, with_info=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "1d0327a5",
      "metadata": {
        "id": "1d0327a5"
      },
      "outputs": [],
      "source": [
        "n_classes = info.features[\"label\"].num_classes\n",
        "dataset_size = info.splits[\"train\"].num_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "fb095b96",
      "metadata": {
        "id": "fb095b96"
      },
      "outputs": [],
      "source": [
        "test_set_raw, valid_set_raw, train_set_raw = tfds.load(\n",
        "    \"tf_flowers\",\n",
        "    split=[\"train[:10%]\", \"train[10%:25%]\", \"train[25%:]\"],\n",
        "    as_supervised=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing steps"
      ],
      "metadata": {
        "id": "PFjXtjJnQoXl"
      },
      "id": "PFjXtjJnQoXl"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "bdb1412a",
      "metadata": {
        "id": "bdb1412a"
      },
      "outputs": [],
      "source": [
        "def preprocess(image, label):\n",
        "    resized_image = tf.image.resize(image, [224, 224])\n",
        "    final_image = keras.applications.xception.preprocess_input(resized_image)\n",
        "    return final_image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "ac4a97b3",
      "metadata": {
        "id": "ac4a97b3"
      },
      "outputs": [],
      "source": [
        "def central_crop(image):\n",
        "    shape = tf.shape(image)\n",
        "    min_dim = tf.reduce_min([shape[0], shape[1]])\n",
        "    top_crop = (shape[0] - min_dim) // 4\n",
        "    bottom_crop = shape[0] - top_crop\n",
        "    left_crop = (shape[1] - min_dim) // 4\n",
        "    right_crop = shape[1] - left_crop\n",
        "    return image[top_crop:bottom_crop, left_crop:right_crop]\n",
        "\n",
        "def random_crop(image):\n",
        "    shape = tf.shape(image)\n",
        "    min_dim = tf.reduce_min([shape[0], shape[1]]) * 90 // 100\n",
        "    return tf.image.random_crop(image, [min_dim, min_dim, 3])\n",
        "\n",
        "def preprocess(image, label, randomize=False):\n",
        "    if randomize:\n",
        "        cropped_image = random_crop(image)\n",
        "        cropped_image = tf.image.random_flip_left_right(cropped_image)\n",
        "    else:\n",
        "        cropped_image = central_crop(image)\n",
        "    resized_image = tf.image.resize(cropped_image, [224, 224])\n",
        "    final_image = keras.applications.xception.preprocess_input(resized_image)\n",
        "    return final_image, label\n",
        "\n",
        "batch_size = 32\n",
        "train_set = train_set_raw.shuffle(1000).repeat()\n",
        "train_set = train_set.map(preprocess).batch(batch_size).prefetch(1)\n",
        "valid_set = valid_set_raw.map(preprocess).batch(batch_size).prefetch(1)\n",
        "test_set = test_set_raw.map(preprocess).batch(batch_size).prefetch(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning and Training"
      ],
      "metadata": {
        "id": "ABcdLlvfQvQ0"
      },
      "id": "ABcdLlvfQvQ0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Xception model"
      ],
      "metadata": {
        "id": "f5noFjbGQ0j2"
      },
      "id": "f5noFjbGQ0j2"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "a76cdaf0",
      "metadata": {
        "id": "a76cdaf0"
      },
      "outputs": [],
      "source": [
        "base_model = keras.applications.xception.Xception(weights=\"imagenet\",\n",
        "                                                  include_top=False)\n",
        "avg = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
        "output = keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\n",
        "model = keras.models.Model(inputs=base_model.input, outputs=output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "f1f2890d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1f2890d",
        "outputId": "7ad99161-35fe-4e18-f395-e9a74dfdc73c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "86/86 [==============================] - 153s 2s/step - loss: 0.5667 - accuracy: 0.7980 - val_loss: 0.3153 - val_accuracy: 0.8860\n",
            "Epoch 2/10\n",
            "86/86 [==============================] - 113s 1s/step - loss: 0.1138 - accuracy: 0.9677 - val_loss: 0.2117 - val_accuracy: 0.9154\n",
            "Epoch 3/10\n",
            "86/86 [==============================] - 113s 1s/step - loss: 0.0426 - accuracy: 0.9909 - val_loss: 0.2042 - val_accuracy: 0.9283\n",
            "Epoch 4/10\n",
            "86/86 [==============================] - 113s 1s/step - loss: 0.0196 - accuracy: 0.9967 - val_loss: 0.2051 - val_accuracy: 0.9338\n",
            "Epoch 5/10\n",
            "86/86 [==============================] - 113s 1s/step - loss: 0.0142 - accuracy: 0.9971 - val_loss: 0.2173 - val_accuracy: 0.9246\n",
            "Epoch 6/10\n",
            "86/86 [==============================] - 113s 1s/step - loss: 0.0107 - accuracy: 0.9982 - val_loss: 0.2370 - val_accuracy: 0.9210\n",
            "Epoch 7/10\n",
            "86/86 [==============================] - 114s 1s/step - loss: 0.0088 - accuracy: 0.9989 - val_loss: 0.2381 - val_accuracy: 0.9210\n",
            "Epoch 8/10\n",
            "86/86 [==============================] - 114s 1s/step - loss: 0.0066 - accuracy: 0.9989 - val_loss: 0.2211 - val_accuracy: 0.9338\n",
            "Epoch 9/10\n",
            "86/86 [==============================] - 114s 1s/step - loss: 0.0061 - accuracy: 0.9993 - val_loss: 0.2133 - val_accuracy: 0.9393\n",
            "Epoch 10/10\n",
            "86/86 [==============================] - 114s 1s/step - loss: 0.0045 - accuracy: 0.9993 - val_loss: 0.2310 - val_accuracy: 0.9320\n"
          ]
        }
      ],
      "source": [
        "for layer in base_model.layers:\n",
        "    layer.trainable = True\n",
        "\n",
        "optimizer = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9,\n",
        "                                 nesterov=True, decay=0.001)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
        "              metrics=[\"accuracy\"])\n",
        "history = model.fit(train_set,\n",
        "                    steps_per_epoch=int(0.75 * dataset_size / batch_size),\n",
        "                    validation_data=valid_set,\n",
        "                    validation_steps=int(0.15 * dataset_size / batch_size),\n",
        "                    epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize layers of transfered model"
      ],
      "metadata": {
        "id": "pQ2IbUOJRDQh"
      },
      "id": "pQ2IbUOJRDQh"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "4bceacde",
      "metadata": {
        "id": "4bceacde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6958d78-7f8a-4866-b644-abaab7a685b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 input_2\n",
            "1 block1_conv1\n",
            "2 block1_conv1_bn\n",
            "3 block1_conv1_act\n",
            "4 block1_conv2\n",
            "5 block1_conv2_bn\n",
            "6 block1_conv2_act\n",
            "7 block2_sepconv1\n",
            "8 block2_sepconv1_bn\n",
            "9 block2_sepconv2_act\n",
            "10 block2_sepconv2\n",
            "11 block2_sepconv2_bn\n",
            "12 conv2d_4\n",
            "13 block2_pool\n",
            "14 batch_normalization_4\n",
            "15 add_12\n",
            "16 block3_sepconv1_act\n",
            "17 block3_sepconv1\n",
            "18 block3_sepconv1_bn\n",
            "19 block3_sepconv2_act\n",
            "20 block3_sepconv2\n",
            "21 block3_sepconv2_bn\n",
            "22 conv2d_5\n",
            "23 block3_pool\n",
            "24 batch_normalization_5\n",
            "25 add_13\n",
            "26 block4_sepconv1_act\n",
            "27 block4_sepconv1\n",
            "28 block4_sepconv1_bn\n",
            "29 block4_sepconv2_act\n",
            "30 block4_sepconv2\n",
            "31 block4_sepconv2_bn\n",
            "32 conv2d_6\n",
            "33 block4_pool\n",
            "34 batch_normalization_6\n",
            "35 add_14\n",
            "36 block5_sepconv1_act\n",
            "37 block5_sepconv1\n",
            "38 block5_sepconv1_bn\n",
            "39 block5_sepconv2_act\n",
            "40 block5_sepconv2\n",
            "41 block5_sepconv2_bn\n",
            "42 block5_sepconv3_act\n",
            "43 block5_sepconv3\n",
            "44 block5_sepconv3_bn\n",
            "45 add_15\n",
            "46 block6_sepconv1_act\n",
            "47 block6_sepconv1\n",
            "48 block6_sepconv1_bn\n",
            "49 block6_sepconv2_act\n",
            "50 block6_sepconv2\n",
            "51 block6_sepconv2_bn\n",
            "52 block6_sepconv3_act\n",
            "53 block6_sepconv3\n",
            "54 block6_sepconv3_bn\n",
            "55 add_16\n",
            "56 block7_sepconv1_act\n",
            "57 block7_sepconv1\n",
            "58 block7_sepconv1_bn\n",
            "59 block7_sepconv2_act\n",
            "60 block7_sepconv2\n",
            "61 block7_sepconv2_bn\n",
            "62 block7_sepconv3_act\n",
            "63 block7_sepconv3\n",
            "64 block7_sepconv3_bn\n",
            "65 add_17\n",
            "66 block8_sepconv1_act\n",
            "67 block8_sepconv1\n",
            "68 block8_sepconv1_bn\n",
            "69 block8_sepconv2_act\n",
            "70 block8_sepconv2\n",
            "71 block8_sepconv2_bn\n",
            "72 block8_sepconv3_act\n",
            "73 block8_sepconv3\n",
            "74 block8_sepconv3_bn\n",
            "75 add_18\n",
            "76 block9_sepconv1_act\n",
            "77 block9_sepconv1\n",
            "78 block9_sepconv1_bn\n",
            "79 block9_sepconv2_act\n",
            "80 block9_sepconv2\n",
            "81 block9_sepconv2_bn\n",
            "82 block9_sepconv3_act\n",
            "83 block9_sepconv3\n",
            "84 block9_sepconv3_bn\n",
            "85 add_19\n",
            "86 block10_sepconv1_act\n",
            "87 block10_sepconv1\n",
            "88 block10_sepconv1_bn\n",
            "89 block10_sepconv2_act\n",
            "90 block10_sepconv2\n",
            "91 block10_sepconv2_bn\n",
            "92 block10_sepconv3_act\n",
            "93 block10_sepconv3\n",
            "94 block10_sepconv3_bn\n",
            "95 add_20\n",
            "96 block11_sepconv1_act\n",
            "97 block11_sepconv1\n",
            "98 block11_sepconv1_bn\n",
            "99 block11_sepconv2_act\n",
            "100 block11_sepconv2\n",
            "101 block11_sepconv2_bn\n",
            "102 block11_sepconv3_act\n",
            "103 block11_sepconv3\n",
            "104 block11_sepconv3_bn\n",
            "105 add_21\n",
            "106 block12_sepconv1_act\n",
            "107 block12_sepconv1\n",
            "108 block12_sepconv1_bn\n",
            "109 block12_sepconv2_act\n",
            "110 block12_sepconv2\n",
            "111 block12_sepconv2_bn\n",
            "112 block12_sepconv3_act\n",
            "113 block12_sepconv3\n",
            "114 block12_sepconv3_bn\n",
            "115 add_22\n",
            "116 block13_sepconv1_act\n",
            "117 block13_sepconv1\n",
            "118 block13_sepconv1_bn\n",
            "119 block13_sepconv2_act\n",
            "120 block13_sepconv2\n",
            "121 block13_sepconv2_bn\n",
            "122 conv2d_7\n",
            "123 block13_pool\n",
            "124 batch_normalization_7\n",
            "125 add_23\n",
            "126 block14_sepconv1\n",
            "127 block14_sepconv1_bn\n",
            "128 block14_sepconv1_act\n",
            "129 block14_sepconv2\n",
            "130 block14_sepconv2_bn\n",
            "131 block14_sepconv2_act\n"
          ]
        }
      ],
      "source": [
        "for index, layer in enumerate(base_model.layers):\n",
        "    print(index, layer.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EffcientNet B0 model"
      ],
      "metadata": {
        "id": "lejsYApEQ5jQ"
      },
      "id": "lejsYApEQ5jQ"
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = keras.applications.EfficientNetB0(weights=\"imagenet\",\n",
        "                                                  include_top=False)\n",
        "avg = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
        "output = keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\n",
        "model = keras.models.Model(inputs=base_model.input, outputs=output)"
      ],
      "metadata": {
        "id": "NhzB1y6sP64a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a40c1c74-dd1b-4d55-e26a-79a91a5c2936"
      },
      "id": "NhzB1y6sP64a",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "16711680/16705208 [==============================] - 0s 0us/step\n",
            "16719872/16705208 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in base_model.layers:\n",
        "    layer.trainable = True\n",
        "\n",
        "optimizer = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9,\n",
        "                                 nesterov=True, decay=0.001)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
        "              metrics=[\"accuracy\"])\n",
        "history = model.fit(train_set,\n",
        "                    steps_per_epoch=int(0.75 * dataset_size / batch_size),\n",
        "                    validation_data=valid_set,\n",
        "                    validation_steps=int(0.15 * dataset_size / batch_size),\n",
        "                    epochs=10)"
      ],
      "metadata": {
        "id": "rlOzKRThP-Li"
      },
      "id": "rlOzKRThP-Li",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize layers of transfered model"
      ],
      "metadata": {
        "id": "DPt3syIYRKmf"
      },
      "id": "DPt3syIYRKmf"
    },
    {
      "cell_type": "code",
      "source": [
        "for index, layer in enumerate(base_model.layers):\n",
        "    print(index, layer.name)"
      ],
      "metadata": {
        "id": "RT03vO3TQLjk"
      },
      "id": "RT03vO3TQLjk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNet 101 model"
      ],
      "metadata": {
        "id": "XoiHuyLIQ9aH"
      },
      "id": "XoiHuyLIQ9aH"
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = keras.applications.ResNet101(weights=\"imagenet\",\n",
        "                                                  include_top=False)\n",
        "avg = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
        "output = keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\n",
        "model = keras.models.Model(inputs=base_model.input, outputs=output)"
      ],
      "metadata": {
        "id": "RGryct_kQNm3"
      },
      "id": "RGryct_kQNm3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in base_model.layers:\n",
        "    layer.trainable = True\n",
        "\n",
        "optimizer = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9,\n",
        "                                 nesterov=True, decay=0.001)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
        "              metrics=[\"accuracy\"])\n",
        "history = model.fit(train_set,\n",
        "                    steps_per_epoch=int(0.75 * dataset_size / batch_size),\n",
        "                    validation_data=valid_set,\n",
        "                    validation_steps=int(0.15 * dataset_size / batch_size),\n",
        "                    epochs=10)"
      ],
      "metadata": {
        "id": "IAcX8n7aQQpS"
      },
      "id": "IAcX8n7aQQpS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize layers of transfered model"
      ],
      "metadata": {
        "id": "OoMF4TQuRM-f"
      },
      "id": "OoMF4TQuRM-f"
    },
    {
      "cell_type": "code",
      "source": [
        "for index, layer in enumerate(base_model.layers):\n",
        "    print(index, layer.name)"
      ],
      "metadata": {
        "id": "pkj8FJywQRUE"
      },
      "id": "pkj8FJywQRUE",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "Lab10.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}