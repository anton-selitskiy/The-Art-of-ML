{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab11_light.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN8EmYCUkKZJQjI5+AqAzyf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"sDhldrJR-8Du"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SNvkUmD5Dcxv"},"outputs":[],"source":["import numpy as np\n","import sklearn\n","import tensorflow as tf\n","from tensorflow import keras\n","%matplotlib inline\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import tensorflow_datasets as tfds\n","import os\n","from pathlib import Path"]},{"cell_type":"markdown","source":["## Example1: Name - country classification\n","\n","https://youtu.be/WEV61GmmPrk"],"metadata":{"id":"nZlhLPNOkAwL"}},{"cell_type":"code","source":["#dataset\n","!wget \"https://download.pytorch.org/tutorial/data.zip\""],"metadata":{"id":"1rYmaJo0_Ctd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#utilities to upload data\n","!wget \"https://raw.githubusercontent.com/python-engineer/pytorch-examples/master/rnn-name-classification/utils.py\""],"metadata":{"id":"xA028jhR_FLS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!unzip data.zip"],"metadata":{"id":"nagW_yiV_HdK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"7jbb7qSe_KlO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from utils import ALL_LETTERS, N_LETTERS\n","from utils import load_data #, letter_to_tensor, line_to_tensor, random_training_example, unicode_to_ascii"],"metadata":{"id":"25Deltd9hcSY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ALL_LETTERS"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"tRhTbBde7fUa","executionInfo":{"status":"ok","timestamp":1649344376090,"user_tz":240,"elapsed":143,"user":{"displayName":"Anton Selitskii","userId":"15865143011662476323"}},"outputId":"82403f0b-7345-451f-cd33-11341ec95e60"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ .,;'\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["category_lines, all_categories = load_data()\n","n_categories = len(all_categories)"],"metadata":{"id":"1IbUmTNJiY7K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["list('Albert')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bgRfRfoL8Fay","executionInfo":{"status":"ok","timestamp":1649344386917,"user_tz":240,"elapsed":149,"user":{"displayName":"Anton Selitskii","userId":"15865143011662476323"}},"outputId":"573aec03-c910-40c1-ae24-29cb0f0c4b8b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['A', 'l', 'b', 'e', 'r', 't']"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["depth = len(ALL_LETTERS)\n","indices = [ALL_LETTERS.find(letter) for letter in 'Albert']\n","indices"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qELy-7lW8LzZ","executionInfo":{"status":"ok","timestamp":1649344394636,"user_tz":240,"elapsed":156,"user":{"displayName":"Anton Selitskii","userId":"15865143011662476323"}},"outputId":"99bb30dc-8784-48be-8556-946fd3356acf"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[26, 11, 1, 4, 17, 19]"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["tf.one_hot(indices, depth)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I15hlOR-Tzch","executionInfo":{"status":"ok","timestamp":1649344401955,"user_tz":240,"elapsed":347,"user":{"displayName":"Anton Selitskii","userId":"15865143011662476323"}},"outputId":"374990ab-9559-4c81-8497-1bc5597e6d8b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(6, 57), dtype=float32, numpy=\n","array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)>"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["X = []\n","y = []\n","for i in range(len(all_categories)):\n","  X = X + category_lines[all_categories[i]]\n","  y = y+[i]*len(category_lines[all_categories[i]])"],"metadata":{"id":"KlCLrUyQqLF3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"o6vzDXN0Yuib"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split"],"metadata":{"id":"Na7lfTlsUbAu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"],"metadata":{"id":"qMZDmZ-2UcfO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#See the data\n","X_train[:4], [all_categories[i] for i in y_train[:4]]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2hCN_HZvUpTv","executionInfo":{"status":"ok","timestamp":1649344450179,"user_tz":240,"elapsed":169,"user":{"displayName":"Anton Selitskii","userId":"15865143011662476323"}},"outputId":"b2fa0260-8eba-4830-f0a3-ec5a7cc519e6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["([\"O'Leary\", 'Shammas', 'Ravenscroft', 'Colon'],\n"," ['Irish', 'Arabic', 'English', 'Spanish'])"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["#Create a look up table\n","vocab = list(ALL_LETTERS)\n","indices = tf.range(len(vocab), dtype=tf.int64)\n","table_init = tf.lookup.KeyValueTensorInitializer(vocab, indices)\n","table = tf.lookup.StaticVocabularyTable(table_init, num_oov_buckets=1)"],"metadata":{"id":"VJ8-LEypWDNU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tf.expand_dims(tf.one_hot(table.lookup(tf.constant(list(X_train[0]))), depth=len(vocab)), axis=0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EPM70YQtbbui","executionInfo":{"status":"ok","timestamp":1649344490321,"user_tz":240,"elapsed":166,"user":{"displayName":"Anton Selitskii","userId":"15865143011662476323"}},"outputId":"f7de9a0f-856b-4dd0-dcbf-a2fcdf5309d1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1, 7, 57), dtype=float32, numpy=\n","array([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 1.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 0.]]], dtype=float32)>"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["model = keras.Sequential([keras.layers.GRU(len(vocab)),\n","    keras.layers.Dense(len(all_categories), activation=\"softmax\")])"],"metadata":{"id":"K28p2nXQh1_k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tf.argmax(model(tf.expand_dims(tf.one_hot(table.lookup(tf.constant(list(X_train[0]))), depth=len(vocab)), axis=0)), axis=1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I_rFTjGFigJE","executionInfo":{"status":"ok","timestamp":1649295300072,"user_tz":240,"elapsed":3333,"user":{"displayName":"Anton Selitskii","userId":"15865143011662476323"}},"outputId":"3b456fed-65a3-4a1f-b8ba-659986090def"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1,), dtype=int64, numpy=array([8])>"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["optimizer = keras.optimizers.Adam()"],"metadata":{"id":"y5er69d0ety6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"UInN89NHp5nd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loss_fn = keras.losses.categorical_crossentropy"],"metadata":{"id":"mbvigqJGe2ch"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["metrics = [keras.metrics.categorical_accuracy]"],"metadata":{"id":"fNJDqvGvfFo4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epochs = 10\n","n_steps = len(X_train)\n","batch_size = 1"],"metadata":{"id":"mM5aiSErgKhT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mean_loss = keras.metrics.Mean()"],"metadata":{"id":"B3LYF-8Lj10G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Source: https://github.com/ageron/handson-ml2/blob/master/12_custom_models_and_training_with_tensorflow.ipynb\n","def progress_bar(iteration, total, size=30):\n","    running = iteration < total\n","    c = \">\" if running else \"=\"\n","    p = (size - 1) * iteration // total\n","    fmt = \"{{:-{}d}}/{{}} [{{}}]\".format(len(str(total)))\n","    params = [iteration, total, \"=\" * p + c + \".\" * (size - p - 1)]\n","    return fmt.format(*params)\n","\n","def print_status_bar(iteration, total, loss, metrics=None, size=30):\n","    metrics = \" - \".join([\"{}: {:.4f}\".format('acc', m.result()) for m in [loss]])\n","    end = \"\" if iteration < total else \"\\n\"\n","    print(\"\\r{} - {}\".format(progress_bar(iteration, total), metrics), end=end)"],"metadata":{"id":"Cboj0V9BkR68"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for epoch in range(1, epochs + 1):\n","    print(\"Epoch {}/{}\".format(epoch, epochs))\n","    for step in range(1, n_steps + 1):\n","        idx = np.random.randint(n_steps)\n","        X_batch, y_batch = tf.expand_dims(tf.one_hot(table.lookup(tf.constant(list(X_train[idx]))), depth=len(vocab)), axis=0), tf.expand_dims(tf.one_hot(y_train[idx], depth=len(all_categories)), axis=0)\n","        with tf.GradientTape() as tape:\n","            y_pred = model(X_batch) #all_categories, axis=1)\n","            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n","            loss = tf.add_n([main_loss] + model.losses)\n","        gradients = tape.gradient(loss, model.trainable_variables)\n","        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","        for variable in model.variables:\n","            if variable.constraint is not None:\n","                variable.assign(variable.constraint(variable))\n","        mean_loss(loss)\n","        for metric in metrics:\n","            metric(y_batch, y_pred)\n","        print_status_bar(step * batch_size, len(y_train), mean_loss, metrics)\n","    print_status_bar(len(y_train), len(y_train), mean_loss, metrics)\n","    for metric in [mean_loss] + metrics:\n","        metric.reset_states()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":455},"id":"MM78aIhGfW0P","executionInfo":{"status":"error","timestamp":1649295551501,"user_tz":240,"elapsed":238411,"user":{"displayName":"Anton Selitskii","userId":"15865143011662476323"}},"outputId":"3786e156-e2a7-4a32-a87b-af05be3df1e5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","12350/16059 [======================>.......] - acc: 1.0629"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-180452b93170>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mmain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmain_loss\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mvariable\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1087\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1088\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     71\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    154\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/cudnn_rnn_grad.py\u001b[0m in \u001b[0;36m_cudnn_rnn_backward\u001b[0;34m(op, *grads)\u001b[0m\n\u001b[1;32m     40\u001b[0m       \u001b[0mseed2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"seed2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0mrnn_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rnn_mode\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m       \u001b[0minput_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"input_mode\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m       direction=op.get_attr(\"direction\"))\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mget_attr\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mtyp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop_attr_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmake_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["#Test:\n","idx = 6\n","tf.argmax(model(tf.expand_dims(tf.one_hot(table.lookup(tf.constant(list(X_test[idx]))), depth=len(vocab)), axis=0)), axis=1), y_test[idx]"],"metadata":{"id":"yHCRoN8W_YAG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"0AtAckMW_cNZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Example 2: Sentiment Analyisis\n","https://www.kaggle.com/datasets/arkhoshghalb/twitter-sentiment-analysis-hatred-speech"],"metadata":{"id":"TlcLusKsuUfe"}},{"cell_type":"code","source":["!unzip train.csv.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9tw-0tnSuUCe","executionInfo":{"status":"ok","timestamp":1649345119925,"user_tz":240,"elapsed":387,"user":{"displayName":"Anton Selitskii","userId":"15865143011662476323"}},"outputId":"eb9a9773-b324-497e-8d23-9c4182c0ae2e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  train.csv.zip\n","  inflating: train.csv               \n"]}]},{"cell_type":"code","source":["df = pd.read_csv('train.csv')"],"metadata":{"id":"tDVKAzVfur9X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"L91tkdpCu5UV","executionInfo":{"status":"ok","timestamp":1649345127732,"user_tz":240,"elapsed":171,"user":{"displayName":"Anton Selitskii","userId":"15865143011662476323"}},"outputId":"06d61c73-0daf-403a-baf7-dd2b42484ca6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   id  label                                              tweet\n","0   1      0   @user when a father is dysfunctional and is s...\n","1   2      0  @user @user thanks for #lyft credit i can't us...\n","2   3      0                                bihday your majesty\n","3   4      0  #model   i love u take with u all the time in ...\n","4   5      0             factsguide: society now    #motivation"],"text/html":["\n","  <div id=\"df-fcce8504-f2c7-4530-bbf5-bd7140920f4e\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>label</th>\n","      <th>tweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>@user when a father is dysfunctional and is s...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>@user @user thanks for #lyft credit i can't us...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>bihday your majesty</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>#model   i love u take with u all the time in ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>factsguide: society now    #motivation</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fcce8504-f2c7-4530-bbf5-bd7140920f4e')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-fcce8504-f2c7-4530-bbf5-bd7140920f4e button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-fcce8504-f2c7-4530-bbf5-bd7140920f4e');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["text_data = tf.data.Dataset.from_tensor_slices((df.tweet, df.label))"],"metadata":{"id":"sO8GNv3-qESw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocab_size = 20000 \n","index_from = 3\n","seq_len = 32 # 200\n","skip_top=3"],"metadata":{"id":"NTUzJEVmEaFn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"],"metadata":{"id":"3QN5ohDUKrU7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vectorize_layer = TextVectorization(max_tokens=vocab_size, output_sequence_length=seq_len)"],"metadata":{"id":"cVfoA782JjwN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vectorize_layer.adapt(df.tweet)"],"metadata":{"id":"r8sAyHjZJ0ab"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for element, _ in text_data.batch(2).take(1):\n","  print(vectorize_layer(element), element)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FfgLLRZ1Myzr","executionInfo":{"status":"ok","timestamp":1649345724059,"user_tz":240,"elapsed":184,"user":{"displayName":"Anton Selitskii","userId":"15865143011662476323"}},"outputId":"5081fe14-5ab6-40ce-b35b-a7135e73b76d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[    2    35     5   261    11     1     8    11    21  3399   101  7558\n","     94   256   253    94  9702   472     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]\n"," [    2     2   175    10  5982  2658     6    65   435   637    72    70\n","   1522  8166 11028     9  8863     1 14234     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]], shape=(2, 32), dtype=int64) tf.Tensor(\n","[b' @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run'\n"," b\"@user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked\"], shape=(2,), dtype=string)\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"aiD7LVca_hb1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = keras.models.Sequential([vectorize_layer, \n","    tf.keras.layers.Embedding(vocab_size,128),\n","    #tf.keras.layers.GRU(128, return_sequences=True),\n","    tf.keras.layers.GRU(128),\n","    tf.keras.layers.Dense(1, activation=\"sigmoid\")])"],"metadata":{"id":"LdAtFRt9KCCh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#['text_vectorization_1', 'embedding_1', 'gru_1', 'dense_1']\n","#model.get_layer('text_vectorization_1')"],"metadata":{"id":"FZR9oFcMVFrz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for element, _ in text_data.batch(2).take(1):\n","  print(model.predict(element))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TTcbjgNnVApT","executionInfo":{"status":"ok","timestamp":1649346856332,"user_tz":240,"elapsed":2895,"user":{"displayName":"Anton Selitskii","userId":"15865143011662476323"}},"outputId":"aac406d9-1cc6-4226-8b62-f3e7a103be2c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.5144014]\n"," [0.5143914]]\n"]}]},{"cell_type":"code","source":["model.compile(loss='binary_crossentropy', optimizer='adam', metrics='accuracy')"],"metadata":{"id":"dqVD-JtJTITW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history = model.fit(text_data.batch(20), epochs=5)"],"metadata":{"id":"v36QxWtW_mbP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"VMTVN9E1_oaS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Deep RNNs with Layer Norm\""],"metadata":{"id":"ut3yF3RvvJ03"}},{"cell_type":"code","source":[""],"metadata":{"id":"FNtu4OOAvIz8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Example 3: Bach Chorale\n","https://github.com/ageron/handson-ml2/blob/master/15_processing_sequences_using_rnns_and_cnns.ipynb"],"metadata":{"id":"naDMA_x3rLLd"}},{"cell_type":"code","source":[""],"metadata":{"id":"JNfO6dy8S0h6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CaS2Jgj5pxXG"},"source":["## 10. Bach Chorales\n","_Exercise: Download the [Bach chorales](https://homl.info/bach) dataset and unzip it. It is composed of 382 chorales composed by Johann Sebastian Bach. Each chorale is 100 to 640 time steps long, and each time step contains 4 integers, where each integer corresponds to a note's index on a piano (except for the value 0, which means that no note is played). Train a model—recurrent, convolutional, or both—that can predict the next time step (four notes), given a sequence of time steps from a chorale. Then use this model to generate Bach-like music, one note at a time: you can do this by giving the model the start of a chorale and asking it to predict the next time step, then appending these time steps to the input sequence and asking the model for the next note, and so on. Also make sure to check out [Google's Coconet model](https://homl.info/coconet), which was used for a nice [Google doodle about Bach](https://www.google.com/doodles/celebrating-johann-sebastian-bach)._\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l7b4TI8WpxXG"},"outputs":[],"source":["DOWNLOAD_ROOT = \"https://github.com/ageron/handson-ml2/raw/master/datasets/jsb_chorales/\"\n","FILENAME = \"jsb_chorales.tgz\"\n","filepath = keras.utils.get_file(FILENAME,\n","                                DOWNLOAD_ROOT + FILENAME,\n","                                cache_subdir=\"datasets/jsb_chorales\",\n","                                extract=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0CqXH57JpxXG"},"outputs":[],"source":["jsb_chorales_dir = Path(filepath).parent\n","train_files = sorted(jsb_chorales_dir.glob(\"train/chorale_*.csv\"))\n","valid_files = sorted(jsb_chorales_dir.glob(\"valid/chorale_*.csv\"))\n","test_files = sorted(jsb_chorales_dir.glob(\"test/chorale_*.csv\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DNmn8GCzpxXG"},"outputs":[],"source":["def load_chorales(filepaths):\n","    return [pd.read_csv(filepath).values.tolist() for filepath in filepaths]\n","\n","train_chorales = load_chorales(train_files)\n","valid_chorales = load_chorales(valid_files)\n","test_chorales = load_chorales(test_files)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yynYe64KpxXH","outputId":"81cb0c41-992a-41fc-e0e6-cf7757dcbb55","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649346559167,"user_tz":240,"elapsed":3,"user":{"displayName":"Anton Selitskii","userId":"15865143011662476323"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[74, 70, 65, 58],\n"," [74, 70, 65, 58],\n"," [74, 70, 65, 58],\n"," [74, 70, 65, 58],\n"," [75, 70, 58, 55],\n"," [75, 70, 58, 55],\n"," [75, 70, 60, 55],\n"," [75, 70, 60, 55],\n"," [77, 69, 62, 50],\n"," [77, 69, 62, 50],\n"," [77, 69, 62, 50],\n"," [77, 69, 62, 50],\n"," [77, 70, 62, 55],\n"," [77, 70, 62, 55],\n"," [77, 69, 62, 55],\n"," [77, 69, 62, 55],\n"," [75, 67, 63, 48],\n"," [75, 67, 63, 48],\n"," [75, 69, 63, 48],\n"," [75, 69, 63, 48],\n"," [74, 70, 65, 46],\n"," [74, 70, 65, 46],\n"," [74, 70, 65, 46],\n"," [74, 70, 65, 46],\n"," [72, 69, 65, 53],\n"," [72, 69, 65, 53],\n"," [72, 69, 65, 53],\n"," [72, 69, 65, 53],\n"," [72, 69, 65, 53],\n"," [72, 69, 65, 53],\n"," [72, 69, 65, 53],\n"," [72, 69, 65, 53],\n"," [74, 70, 65, 46],\n"," [74, 70, 65, 46],\n"," [74, 70, 65, 46],\n"," [74, 70, 65, 46],\n"," [75, 69, 63, 48],\n"," [75, 69, 63, 48],\n"," [75, 67, 63, 48],\n"," [75, 67, 63, 48],\n"," [77, 65, 62, 50],\n"," [77, 65, 62, 50],\n"," [77, 65, 60, 50],\n"," [77, 65, 60, 50],\n"," [74, 67, 58, 55],\n"," [74, 67, 58, 55],\n"," [74, 67, 58, 53],\n"," [74, 67, 58, 53],\n"," [72, 67, 58, 51],\n"," [72, 67, 58, 51],\n"," [72, 67, 58, 51],\n"," [72, 67, 58, 51],\n"," [72, 65, 57, 53],\n"," [72, 65, 57, 53],\n"," [72, 65, 57, 53],\n"," [72, 65, 57, 53],\n"," [70, 65, 62, 46],\n"," [70, 65, 62, 46],\n"," [70, 65, 62, 46],\n"," [70, 65, 62, 46],\n"," [70, 65, 62, 46],\n"," [70, 65, 62, 46],\n"," [70, 65, 62, 46],\n"," [70, 65, 62, 46],\n"," [72, 69, 65, 53],\n"," [72, 69, 65, 53],\n"," [72, 69, 65, 53],\n"," [72, 69, 65, 53],\n"," [74, 71, 53, 50],\n"," [74, 71, 53, 50],\n"," [74, 71, 53, 50],\n"," [74, 71, 53, 50],\n"," [75, 72, 55, 48],\n"," [75, 72, 55, 48],\n"," [75, 72, 55, 50],\n"," [75, 72, 55, 50],\n"," [75, 67, 60, 51],\n"," [75, 67, 60, 51],\n"," [75, 67, 60, 53],\n"," [75, 67, 60, 53],\n"," [74, 67, 60, 55],\n"," [74, 67, 60, 55],\n"," [74, 67, 57, 55],\n"," [74, 67, 57, 55],\n"," [74, 65, 59, 43],\n"," [74, 65, 59, 43],\n"," [72, 63, 59, 43],\n"," [72, 63, 59, 43],\n"," [72, 63, 55, 48],\n"," [72, 63, 55, 48],\n"," [72, 63, 55, 48],\n"," [72, 63, 55, 48],\n"," [72, 63, 55, 48],\n"," [72, 63, 55, 48],\n"," [72, 63, 55, 48],\n"," [72, 63, 55, 48],\n"," [75, 67, 60, 60],\n"," [75, 67, 60, 60],\n"," [75, 67, 60, 60],\n"," [75, 67, 60, 60],\n"," [77, 70, 62, 58],\n"," [77, 70, 62, 58],\n"," [77, 70, 62, 56],\n"," [77, 70, 62, 56],\n"," [79, 70, 62, 55],\n"," [79, 70, 62, 55],\n"," [79, 70, 62, 53],\n"," [79, 70, 62, 53],\n"," [79, 70, 63, 51],\n"," [79, 70, 63, 51],\n"," [79, 70, 63, 51],\n"," [79, 70, 63, 51],\n"," [77, 70, 63, 58],\n"," [77, 70, 63, 58],\n"," [77, 70, 60, 58],\n"," [77, 70, 60, 58],\n"," [77, 70, 62, 46],\n"," [77, 70, 62, 46],\n"," [77, 68, 62, 46],\n"," [75, 68, 62, 46],\n"," [75, 67, 58, 51],\n"," [75, 67, 58, 51],\n"," [75, 67, 58, 51],\n"," [75, 67, 58, 51],\n"," [75, 67, 58, 51],\n"," [75, 67, 58, 51],\n"," [75, 67, 58, 51],\n"," [75, 67, 58, 51],\n"," [74, 67, 58, 55],\n"," [74, 67, 58, 55],\n"," [74, 67, 58, 55],\n"," [74, 67, 58, 55],\n"," [75, 67, 58, 53],\n"," [75, 67, 58, 53],\n"," [75, 67, 58, 51],\n"," [75, 67, 58, 51],\n"," [77, 65, 58, 50],\n"," [77, 65, 58, 50],\n"," [77, 65, 56, 50],\n"," [77, 65, 56, 50],\n"," [70, 63, 55, 51],\n"," [70, 63, 55, 51],\n"," [70, 63, 55, 51],\n"," [70, 63, 55, 51],\n"," [75, 65, 60, 45],\n"," [75, 65, 60, 45],\n"," [75, 65, 60, 45],\n"," [75, 65, 60, 45],\n"," [74, 65, 58, 46],\n"," [74, 65, 58, 46],\n"," [74, 65, 58, 46],\n"," [74, 65, 58, 46],\n"," [72, 65, 57, 53],\n"," [72, 65, 57, 53],\n"," [72, 65, 57, 53],\n"," [72, 65, 57, 53],\n"," [72, 65, 57, 53],\n"," [72, 65, 57, 53],\n"," [72, 65, 57, 53],\n"," [72, 65, 57, 53],\n"," [74, 65, 58, 58],\n"," [74, 65, 58, 58],\n"," [74, 65, 58, 58],\n"," [74, 65, 58, 58],\n"," [75, 67, 58, 57],\n"," [75, 67, 58, 57],\n"," [75, 67, 58, 55],\n"," [75, 67, 58, 55],\n"," [77, 65, 60, 57],\n"," [77, 65, 60, 57],\n"," [77, 65, 60, 53],\n"," [77, 65, 60, 53],\n"," [74, 65, 58, 58],\n"," [74, 65, 58, 58],\n"," [74, 65, 58, 58],\n"," [74, 65, 58, 58],\n"," [72, 67, 58, 51],\n"," [72, 67, 58, 51],\n"," [72, 67, 58, 51],\n"," [72, 67, 58, 51],\n"," [72, 65, 57, 53],\n"," [72, 65, 57, 53],\n"," [72, 65, 57, 53],\n"," [72, 65, 57, 53],\n"," [70, 65, 62, 46],\n"," [70, 65, 62, 46],\n"," [70, 65, 62, 46],\n"," [70, 65, 62, 46],\n"," [70, 65, 62, 46],\n"," [70, 65, 62, 46],\n"," [70, 65, 62, 46],\n"," [70, 65, 62, 46]]"]},"metadata":{},"execution_count":51}],"source":["train_chorales[0]"]},{"cell_type":"markdown","metadata":{"id":"TS1dJodtpxXH"},"source":["Notes range from 36 (C1 = C on octave 1) to 81 (A5 = A on octave 5), plus 0 for silence:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xrffTjLypxXH"},"outputs":[],"source":["notes = set()\n","for chorales in (train_chorales, valid_chorales, test_chorales):\n","    for chorale in chorales:\n","        for chord in chorale:\n","            notes |= set(chord)\n","\n","n_notes = len(notes)\n","min_note = min(notes - {0})\n","max_note = max(notes)\n","\n","assert min_note == 36\n","assert max_note == 81"]},{"cell_type":"code","source":[""],"metadata":{"id":"Q8b458bA_x0Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nok_NY6bpxXH"},"source":["Let's write a few functions to listen to these chorales (you don't need to understand the details here, and in fact there are certainly simpler ways to do this, for example using MIDI players, but I just wanted to have a bit of fun writing a synthesizer):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dsk36XR3pxXI"},"outputs":[],"source":["from IPython.display import Audio\n","\n","def notes_to_frequencies(notes):\n","    # Frequency doubles when you go up one octave; there are 12 semi-tones\n","    # per octave; Note A on octave 4 is 440 Hz, and it is note number 69.\n","    return 2 ** ((np.array(notes) - 69) / 12) * 440\n","\n","def frequencies_to_samples(frequencies, tempo, sample_rate):\n","    note_duration = 60 / tempo # the tempo is measured in beats per minutes\n","    # To reduce click sound at every beat, we round the frequencies to try to\n","    # get the samples close to zero at the end of each note.\n","    frequencies = np.round(note_duration * frequencies) / note_duration\n","    n_samples = int(note_duration * sample_rate)\n","    time = np.linspace(0, note_duration, n_samples)\n","    sine_waves = np.sin(2 * np.pi * frequencies.reshape(-1, 1) * time)\n","    # Removing all notes with frequencies ≤ 9 Hz (includes note 0 = silence)\n","    sine_waves *= (frequencies > 9.).reshape(-1, 1)\n","    return sine_waves.reshape(-1)\n","\n","def chords_to_samples(chords, tempo, sample_rate):\n","    freqs = notes_to_frequencies(chords)\n","    freqs = np.r_[freqs, freqs[-1:]] # make last note a bit longer\n","    merged = np.mean([frequencies_to_samples(melody, tempo, sample_rate)\n","                     for melody in freqs.T], axis=0)\n","    n_fade_out_samples = sample_rate * 60 // tempo # fade out last note\n","    fade_out = np.linspace(1., 0., n_fade_out_samples)**2\n","    merged[-n_fade_out_samples:] *= fade_out\n","    return merged\n","\n","def play_chords(chords, tempo=160, amplitude=0.1, sample_rate=44100, filepath=None):\n","    samples = amplitude * chords_to_samples(chords, tempo, sample_rate)\n","    if filepath:\n","        from scipy.io import wavfile\n","        samples = (2**15 * samples).astype(np.int16)\n","        wavfile.write(filepath, sample_rate, samples)\n","        return display(Audio(filepath))\n","    else:\n","        return display(Audio(samples, rate=sample_rate))"]},{"cell_type":"code","source":["play_chords([45, 60, 38, 70], 120, 8000)"],"metadata":{"id":"aK_Gxsf7_16z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now let's listen to a few chorales:"],"metadata":{"id":"9svWYXmMAIDp"}},{"cell_type":"code","source":["for index in range(3):\n","    play_chords(train_chorales[index])"],"metadata":{"id":"ofo9olk-_-lp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"zZApwRQ1__3O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-eSaFty3pxXI"},"source":["Divine! :)"]},{"cell_type":"markdown","metadata":{"id":"H2yPytoppxXI"},"source":["In order to be able to generate new chorales, we want to train a model that can predict the next chord given all the previous chords. If we naively try to predict the next chord in one shot, predicting all 4 notes at once, we run the risk of getting notes that don't go very well together (believe me, I tried). It's much better and simpler to predict one note at a time. So we will need to preprocess every chorale, turning each chord into an arpegio (i.e., a sequence of notes rather than notes played simultaneuously). So each chorale will be a long sequence of notes (rather than chords), and we can just train a model that can predict the next note given all the previous notes. We will use a sequence-to-sequence approach, where we feed a window to the neural net, and it tries to predict that same window shifted one time step into the future.\n","\n","We will also shift the values so that they range from 0 to 46, where 0 represents silence, and values 1 to 46 represent notes 36 (C1) to 81 (A5).\n","\n","And we will train the model on windows of 128 notes (i.e., 32 chords).\n","\n","Since the dataset fits in memory, we could preprocess the chorales in RAM using any Python code we like, but I will demonstrate here how to do all the preprocessing using tf.data (there will be more details about creating windows using tf.data in the next chapter)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"85cUQSI7pxXJ"},"outputs":[],"source":["def create_target(batch):\n","    X = batch[:, :-1]\n","    Y = batch[:, 1:] # predict next note in each arpegio, at each step\n","    return X, Y\n","\n","def preprocess(window):\n","    window = tf.where(window == 0, window, window - min_note + 1) # shift values\n","    return tf.reshape(window, [-1]) # convert to arpegio\n","\n","def bach_dataset(chorales, batch_size=32, shuffle_buffer_size=None,\n","                 window_size=32, window_shift=16, cache=True):\n","    def batch_window(window):\n","        return window.batch(window_size + 1)\n","\n","    def to_windows(chorale):\n","        dataset = tf.data.Dataset.from_tensor_slices(chorale)\n","        dataset = dataset.window(window_size + 1, window_shift, drop_remainder=True)\n","        return dataset.flat_map(batch_window)\n","\n","    chorales = tf.ragged.constant(chorales, ragged_rank=1)\n","    dataset = tf.data.Dataset.from_tensor_slices(chorales)\n","    dataset = dataset.flat_map(to_windows).map(preprocess)\n","    if cache:\n","        dataset = dataset.cache()\n","    if shuffle_buffer_size:\n","        dataset = dataset.shuffle(shuffle_buffer_size)\n","    dataset = dataset.batch(batch_size)\n","    dataset = dataset.map(create_target)\n","    return dataset.prefetch(1)"]},{"cell_type":"markdown","metadata":{"id":"xDXzr6BepxXJ"},"source":["Now let's create the training set, the validation set and the test set:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZNsa0ltVpxXJ"},"outputs":[],"source":["train_set = bach_dataset(train_chorales, shuffle_buffer_size=1000)\n","valid_set = bach_dataset(valid_chorales)\n","test_set = bach_dataset(test_chorales)"]},{"cell_type":"code","source":["for element, y in train_set.batch(2).take(1):\n","  print(element[0], y[0])"],"metadata":{"id":"mJTLkKn8AOBc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"0ZKafZ2hAQIX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hz7fmm6JpxXJ"},"source":["Now let's create the model:\n","\n","* We could feed the note values directly to the model, as floats, but this would probably not give good results. Indeed, the relationships between notes are not that simple: for example, if you replace a C3 with a C4, the melody will still sound fine, even though these notes are 12 semi-tones apart (i.e., one octave). Conversely, if you replace a C3 with a C\\#3, it's very likely that the chord will sound horrible, despite these notes being just next to each other. So we will use an `Embedding` layer to convert each note to a small vector representation (see Chapter 16 for more details on embeddings). We will use 5-dimensional embeddings, so the output of this first layer will have a shape of `[batch_size, window_size, 5]`.\n","* We will then feed this data to a small WaveNet-like neural network, composed of a stack of 4 `Conv1D` layers with doubling dilation rates. We will intersperse these layers with `BatchNormalization` layers for faster better convergence.\n","* Then one `LSTM` layer to try to capture long-term patterns.\n","* And finally a `Dense` layer to produce the final note probabilities. It will predict one probability for each chorale in the batch, for each time step, and for each possible note (including silence). So the output shape will be `[batch_size, window_size, 47]`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_SjHiqN6pxXK","executionInfo":{"status":"ok","timestamp":1649346581572,"user_tz":240,"elapsed":852,"user":{"displayName":"Anton Selitskii","userId":"15865143011662476323"}},"outputId":"8c97a745-b182-4443-e087-17975e9c949e","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_1 (Embedding)     (None, None, 5)           235       \n","                                                                 \n"," conv1d (Conv1D)             (None, None, 32)          352       \n","                                                                 \n"," batch_normalization (BatchN  (None, None, 32)         128       \n"," ormalization)                                                   \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, None, 48)          3120      \n","                                                                 \n"," batch_normalization_1 (Batc  (None, None, 48)         192       \n"," hNormalization)                                                 \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, None, 64)          6208      \n","                                                                 \n"," batch_normalization_2 (Batc  (None, None, 64)         256       \n"," hNormalization)                                                 \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, None, 96)          12384     \n","                                                                 \n"," batch_normalization_3 (Batc  (None, None, 96)         384       \n"," hNormalization)                                                 \n","                                                                 \n"," lstm (LSTM)                 (None, None, 256)         361472    \n","                                                                 \n"," dense_1 (Dense)             (None, None, 47)          12079     \n","                                                                 \n","=================================================================\n","Total params: 396,810\n","Trainable params: 396,330\n","Non-trainable params: 480\n","_________________________________________________________________\n"]}],"source":["n_embedding_dims = 5\n","\n","model = keras.models.Sequential([\n","    keras.layers.Embedding(input_dim=n_notes, output_dim=n_embedding_dims,\n","                           input_shape=[None]),\n","    keras.layers.Conv1D(32, kernel_size=2, padding=\"causal\", activation=\"relu\"),\n","    keras.layers.BatchNormalization(),\n","    keras.layers.Conv1D(48, kernel_size=2, padding=\"causal\", activation=\"relu\", dilation_rate=2),\n","    keras.layers.BatchNormalization(),\n","    keras.layers.Conv1D(64, kernel_size=2, padding=\"causal\", activation=\"relu\", dilation_rate=4),\n","    keras.layers.BatchNormalization(),\n","    keras.layers.Conv1D(96, kernel_size=2, padding=\"causal\", activation=\"relu\", dilation_rate=8),\n","    keras.layers.BatchNormalization(),\n","    keras.layers.LSTM(256, return_sequences=True),\n","    keras.layers.Dense(n_notes, activation=\"softmax\")\n","])\n","\n","model.summary()"]},{"cell_type":"code","source":["for element, y in train_set.take(1):\n","  h = element\n","  for layer in model.layers:\n","    h = layer(h)\n","    print(h.shape)"],"metadata":{"id":"v_VlsGocAUGN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for element, y in train_set.batch(2).take(1):\n","  print(model.get_layer('conv1d')(model.get_layer('embedding')(element)).shape, element.shape)"],"metadata":{"id":"YW_Ywc8mAV4I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we're ready to compile and train the model!"],"metadata":{"id":"0QUNM224Adpg"}},{"cell_type":"code","source":["optimizer = keras.optimizers.Nadam(learning_rate=1e-3)\n","model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n","              metrics=[\"accuracy\"])\n","model.fit(train_set, epochs=20, validation_data=valid_set)"],"metadata":{"id":"QlsyBofTAaCO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"wsMWnthWAdJC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qs0OLViGpxXK"},"source":["I have not done much hyperparameter search, so feel free to iterate on this model now and try to optimize it. For example, you could try removing the `LSTM` layer and replacing it with `Conv1D` layers. You could also play with the number of layers, the learning rate, the optimizer, and so on."]},{"cell_type":"markdown","metadata":{"id":"Mo1tGLt9pxXL"},"source":["Once you're satisfied with the performance of the model on the validation set, you can save it and evaluate it one last time on the test set:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U87OCh0dpxXL","outputId":"c7e00a8f-a6d2-422d-e3d2-5a1f418eeaed","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649346615037,"user_tz":240,"elapsed":6905,"user":{"displayName":"Anton Selitskii","userId":"15865143011662476323"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["34/34 [==============================] - 7s 160ms/step - loss: 3.4866 - accuracy: 0.0616\n"]},{"output_type":"execute_result","data":{"text/plain":["[3.4866228103637695, 0.0616338886320591]"]},"metadata":{},"execution_count":63}],"source":["model.save(\"my_bach_model.h5\")\n","model.evaluate(test_set)"]},{"cell_type":"markdown","metadata":{"id":"La5Ak_zFpxXL"},"source":["**Note:** There's no real need for a test set in this exercise, since we will perform the final evaluation by just listening to the music produced by the model. So if you want, you can add the test set to the train set, and train the model again, hopefully getting a slightly better model."]},{"cell_type":"markdown","metadata":{"id":"SrY8jbVypxXM"},"source":["Now let's write a function that will generate a new chorale. We will give it a few seed chords, it will convert them to arpegios (the format expected by the model), and use the model to predict the next note, then the next, and so on. In the end, it will group the notes 4 by 4 to create chords again, and return the resulting chorale."]},{"cell_type":"markdown","metadata":{"id":"zTJybEFBpxXM"},"source":["**Warning**: `model.predict_classes(X)` is deprecated. It is replaced with `np.argmax(model.predict(X), axis=-1)`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IHM-b2j-pxXM"},"outputs":[],"source":["def generate_chorale(model, seed_chords, length):\n","    arpegio = preprocess(tf.constant(seed_chords, dtype=tf.int64))\n","    arpegio = tf.reshape(arpegio, [1, -1])\n","    for chord in range(length):\n","        for note in range(4):\n","            #next_note = model.predict_classes(arpegio)[:1, -1:]\n","            next_note = np.argmax(model.predict(arpegio), axis=-1)[:1, -1:]\n","            arpegio = tf.concat([arpegio, next_note], axis=1)\n","    arpegio = tf.where(arpegio == 0, arpegio, arpegio + min_note - 1)\n","    return tf.reshape(arpegio, shape=[-1, 4])"]},{"cell_type":"markdown","metadata":{"id":"MMKlK2XZpxXM"},"source":["To test this function, we need some seed chords. Let's use the first 8 chords of one of the test chorales (it's actually just 2 different chords, each played 4 times):"]},{"cell_type":"code","source":["seed_chords = test_chorales[2][:8]\n","play_chords(seed_chords, amplitude=0.2)"],"metadata":{"id":"EDucYC8LAmz5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we are ready to generate our first chorale! Let's ask the function to generate 56 more chords, for a total of 64 chords, i.e., 16 bars (assuming 4 chords per bar, i.e., a 4/4 signature):"],"metadata":{"id":"rMzcFp31AtAS"}},{"cell_type":"code","source":["new_chorale = generate_chorale(model, seed_chords, 56)\n","play_chords(new_chorale)"],"metadata":{"id":"UxgV--kIAolv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"WDxJM-5UAvOY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0UHF9wYupxXN"},"source":["This approach has one major flaw: it is often too conservative. Indeed, the model will not take any risk, it will always choose the note with the highest score, and since repeating the previous note generally sounds good enough, it's the least risky option, so the algorithm will tend to make notes last longer and longer. Pretty boring. Plus, if you run the model multiple times, it will always generate the same melody.\n","\n","So let's spice things up a bit! Instead of always picking the note with the highest score, we will pick the next note randomly, according to the predicted probabilities. For example, if the model predicts a C3 with 75% probability, and a G3 with a 25% probability, then we will pick one of these two notes randomly, with these probabilities. We will also add a `temperature` parameter that will control how \"hot\" (i.e., daring) we want the system to feel. A high temperature will bring the predicted probabilities closer together, reducing the probability of the likely notes and increasing the probability of the unlikely ones."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OSBmjzLPpxXO"},"outputs":[],"source":["def generate_chorale_v2(model, seed_chords, length, temperature=1):\n","    arpegio = preprocess(tf.constant(seed_chords, dtype=tf.int64))\n","    arpegio = tf.reshape(arpegio, [1, -1])\n","    for chord in range(length):\n","        for note in range(4):\n","            next_note_probas = model.predict(arpegio)[0, -1:]\n","            rescaled_logits = tf.math.log(next_note_probas) / temperature\n","            next_note = tf.random.categorical(rescaled_logits, num_samples=1)\n","            arpegio = tf.concat([arpegio, next_note], axis=1)\n","    arpegio = tf.where(arpegio == 0, arpegio, arpegio + min_note - 1)\n","    return tf.reshape(arpegio, shape=[-1, 4])"]},{"cell_type":"markdown","metadata":{"id":"cjwzKo2OpxXO"},"source":["Let's generate 3 chorales using this new function: one cold, one medium, and one hot (feel free to experiment with other seeds, lengths and temperatures). The code saves each chorale to a separate file. You can run these cells over an over again until you generate a masterpiece!\n","\n","**Please share your most beautiful generated chorale with me on Twitter @aureliengeron, I would really appreciate it! :))**"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"OmStODFlpxXO"},"outputs":[],"source":["new_chorale_v2_cold = generate_chorale_v2(model, seed_chords, 56, temperature=0.8)\n","play_chords(new_chorale_v2_cold, filepath=\"bach_cold.wav\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p-SKfi20pxXO"},"outputs":[],"source":["new_chorale_v2_medium = generate_chorale_v2(model, seed_chords, 56, temperature=1.0)\n","play_chords(new_chorale_v2_medium, filepath=\"bach_medium.wav\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ad-JbqM1pxXP"},"outputs":[],"source":["new_chorale_v2_hot = generate_chorale_v2(model, seed_chords, 56, temperature=1.5)\n","play_chords(new_chorale_v2_hot, filepath=\"bach_hot.wav\")"]},{"cell_type":"markdown","metadata":{"id":"OROIzpnipxXP"},"source":["Lastly, you can try a fun social experiment: send your friends a few of your favorite generated chorales, plus the real chorale, and ask them to guess which one is the real one!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W4kGQ1bmpxXP"},"outputs":[],"source":["play_chords(test_chorales[2][:64], filepath=\"bach_test_4.wav\")"]}]}